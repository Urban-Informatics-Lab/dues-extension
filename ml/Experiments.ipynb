{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pyreadr\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, LSTM, TimeDistributed, Dropout, Conv1D, MaxPooling1D, Flatten, RepeatVector\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "import dues_utilities as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(77)\n",
    "os.environ['PYTHONHASHSEED'] = str(77)\n",
    "np.random.seed(77)\n",
    "random.seed(77)\n",
    "\n",
    "# Data read and formatting\n",
    "\n",
    "PATH_ENERGY_FEATURIZED = \"../data/building_energy_featurized.csv\"\n",
    "PATH_DISTANCES = \"D:/smud/smud_distances.csv\"\n",
    "PATH_ENERGY_ACTUAL = \"../data/building_energy_actual.csv\"\n",
    "PATH_ENERGY_SIM = \"../data/building_energy_sim.csv\"\n",
    "\n",
    "energy_dtype = {\n",
    "    'apn': str, \n",
    "    'year': np.float32, \n",
    "    'month': np.float32, \n",
    "    'day': np.float32, \n",
    "    'hour': np.float32, \n",
    "    'kwh': np.float32\n",
    "}\n",
    "\n",
    "energy_featurized = pd.read_csv(PATH_ENERGY_FEATURIZED, dtype=np.float32)\n",
    "dist = pd.read_csv(PATH_DISTANCES, dtype=str)\n",
    "energy_actual = pd.read_csv(PATH_ENERGY_ACTUAL, dtype=energy_dtype)\n",
    "energy_sim = pd.read_csv(PATH_ENERGY_SIM, dtype=energy_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of timesteps before 't' model uses in each prediction step\n",
    "timesteps = 6\n",
    "N_PREV = N_PREV - 1\n",
    "\n",
    "# Combined simulation and observed data into properly formatted dataframe\n",
    "energy = utils.get_energy_df(energy_sim, energy_actual)\n",
    "\n",
    "# Fit Standard Scaler ontraining data\n",
    "standard_scaler = utils.get_standard_scaler(energy, 'year < 2018', 'kwh_actual')\n",
    "\n",
    "# Process energy dataframe for training and validation/test.\n",
    "train_x, train_y = utils.preprocess(energy, 'year < 2018', standard_scaler, n_in=N_PREV, df_name=\"Train\", lstm=False)\n",
    "val_x, val_y = utils.preprocess(energy, 'year >= 2018 and month < 7', standard_scaler, n_in=N_PREV, df_name=\"Validation\", lstm=False)\n",
    "test_x, test_y = utils.preprocess(energy, 'year >= 2018 and month >= 7', standard_scaler, n_in=N_PREV, df_name=\"Test\", lstm=False)\n",
    "\n",
    "n_features = train_x.shape[-1]\n",
    "\n",
    "# Callback Functions\n",
    "early_stopping_monitor = EarlyStopping(patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LinearRegression()\n",
    "lr_model = lr_model.fit(train_x, train_y)\n",
    "\n",
    "utils.print_metrics(lr_model, val_x, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = Sequential()\n",
    "ann_model.add(Dense(128, activation='relu', input_shape = [n_features]))\n",
    "ann_model.add(Dense(32, activation='relu'))\n",
    "ann_model.add(Dense(1))\n",
    "\n",
    "ann_model.compile(optimizer='adam', loss='mean_absolute_percentage_error')\n",
    "ann_model.summary()\n",
    "\n",
    "ann_history = ann_model.fit(\n",
    "    train_x, train_y, \n",
    "    validation_data=[val_x, val_y], \n",
    "    batch_size = 5000, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(ann_history, ann_model, val_x, val_y, \"ANN\")\n",
    "\n",
    "# MAPE: 51.44270658493042\n",
    "# CV(RMSE): 56.72162091525358\n",
    "# MBE: -18.218822"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping for Models with 3D Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = utils.reshape_for_lstm(train_x, timesteps, df_name=\"Train\")\n",
    "val_x = utils.reshape_for_lstm(val_x, timesteps, df_name=\"Validation\")\n",
    "test_x = utils.reshape_for_lstm(test_x, timesteps, df_name=\"Test\")\n",
    "n_features = train_x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(Conv1D(128, kernel_size=2, input_shape = (timesteps, n_features)))\n",
    "lstm_model.add(Flatten())\n",
    "lstm_model.add(Dense(32, activation='relu'))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mape')\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    train_x, train_y, \n",
    "    validation_data=[val_x, val_y],\n",
    "    batch_size = 5000, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(lstm_history, lstm_model, val_x, val_y, \"Vanilla LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = Sequential()\n",
    "cnn_model.add(Conv1D(128, kernel_size=2, input_shape = (timesteps, n_features)))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(32, activation='relu'))\n",
    "cnn_model.add(Dense(1))\n",
    "\n",
    "cnn_model.compile(optimizer='adam', loss='mape')\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_history = cnn_model.fit(\n",
    "    train_x, train_y, \n",
    "    validation_data=[val_x, val_y],\n",
    "    batch_size = 5000, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(cnn_history, cnn_model, val_x, val_y, \"CNN\")\n",
    "\n",
    "# MAPE: 32.95547664165497\n",
    "# CV(RMSE): 43.65593410060542\n",
    "# MBE: -2.0720825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy = utils.get_energy_df(energy_sim, energy_actual, one_hot=False)\n",
    "\n",
    "energy_train = utils.prep_for_seq_lstm(energy, 'year < 2018', timesteps)\n",
    "energy_val = utils.prep_for_seq_lstm(energy, 'year >= 2018 and month < 7', timesteps)\n",
    "energy_test= utils.prep_for_seq_lstm(energy, 'year >= 2018 and month >= 7', timesteps)\n",
    "\n",
    "standard_scaler = utils.preprocessing.StandardScaler()\n",
    "standard_scaler.fit(energy_train.drop(columns='kwh_actual'))\n",
    "\n",
    "train_x, train_y = utils.preprocess(energy_train, None, standard_scaler, n_in=timesteps, df_name=\"Train\", remove_target=False, to_supervised=False)\n",
    "val_x, val_y = utils.preprocess(energy_val, None, standard_scaler, n_in=timesteps, df_name=\"Validation\", remove_target=False, to_supervised=False)\n",
    "test_x, test_y = utils.preprocess(energy_test, None, standard_scaler, n_in=timesteps, df_name=\"Test\", remove_target=False, to_supervised=False)\n",
    "\n",
    "n_features = train_x.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(4, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "lstm_model.add(LSTM(4, activation='relu', return_sequences=True))\n",
    "lstm_model.add(TimeDistributed(Dense(32)))\n",
    "lstm_model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mape')\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_history = lstm_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    validation_data=[val_x, val_y], \n",
    "    batch_size = 30, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(lstm_history, lstm_model, val_x, val_y, \"Sequential Vanilla LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_autoencoder_model = Sequential()\n",
    "lstm_autoencoder_model.add(LSTM(4, activation='relu', input_shape=(timesteps, n_features), return_sequences=False))\n",
    "lstm_autoencoder_model.add(RepeatVector(timesteps))\n",
    "lstm_autoencoder_model.add(LSTM(4, activation='relu', return_sequences=True))\n",
    "lstm_autoencoder_model.add(TimeDistributed(Dense(32)))\n",
    "lstm_autoencoder_model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "lstm_autoencoder_model.compile(optimizer='adam', loss='mape')\n",
    "lstm_autoencoder_model.summary()\n",
    "\n",
    "lstm_autoencoder_history = lstm_autoencoder_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    validation_data=[val_x, val_y], \n",
    "    batch_size = 30, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(lstm_autoencoder, history, lstm_autoencoder_model, val_x, val_y, \"Sequential Seq2Seq LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_model = Sequential()\n",
    "cnn_lstm_model.add(Conv1D(8, activation='relu', kernel_size=2, input_shape=(timesteps, n_features)))\n",
    "#cnn_lstm_model.add(Dropout(0.3))\n",
    "cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_lstm_model.add(Flatten())\n",
    "cnn_lstm_model.add(RepeatVector(timesteps))\n",
    "cnn_lstm_model.add(LSTM(8, activation='relu', return_sequences=True))\n",
    "cnn_lstm_model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "cnn_lstm_model.add(TimeDistributed(Dense(1)))\n",
    "# Above achieves 22% validation MAPE\n",
    "\n",
    "cnn_lstm_model.compile(optimizer='adam', loss='mape')\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "cnn_lstm_model.summary()\n",
    "\n",
    "cnn_lstm_history = cnn_lstm_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    validation_data=[val_x, val_y], \n",
    "    batch_size = 30, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(cnn_lstm_history, cnn_lstm_model, val_x, val_y, \"Sequential CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_dropout_model = Sequential()\n",
    "cnn_lstm_dropout_model.add(Conv1D(8, activation='relu', kernel_size=2, input_shape=(timesteps, n_features)))\n",
    "cnn_lstm_dropout_model.add(Dropout(0.3))\n",
    "cnn_lstm_dropout_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_lstm_dropout_model.add(Flatten())\n",
    "cnn_lstm_dropout_model.add(RepeatVector(timesteps))\n",
    "cnn_lstm_dropout_model.add(LSTM(8, activation='relu', return_sequences=True))\n",
    "cnn_lstm_dropout_model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "cnn_lstm_dropout_model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "cnn_lstm_dropout_model.compile(optimizer='adam', loss='mape')\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "cnn_lstm_dropout_model.summary()\n",
    "\n",
    "cnn_lstm_dropout_history = cnn_lstm_dropout_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    validation_data=[val_x, val_y], \n",
    "    batch_size = 30, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(cnn_lstm_dropout_history, cnn_lstm_dropout_model, val_x, val_y, \"Sequential CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lstm_grad_clipping_model = Sequential()\n",
    "cnn_lstm_grad_clipping_model.add(Conv1D(8, activation='relu', kernel_size=2, input_shape=(timesteps, n_features)))\n",
    "cnn_lstm_grad_clipping_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_lstm_grad_clipping_model.add(Flatten())\n",
    "cnn_lstm_grad_clipping_model.add(RepeatVector(timesteps))\n",
    "cnn_lstm_grad_clipping_model.add(LSTM(8, activation='relu', return_sequences=True))\n",
    "cnn_lstm_grad_clipping_model.add(TimeDistributed(Dense(32, activation='relu')))\n",
    "cnn_lstm_grad_clipping_model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "opt = SGD(lr=0.001, momentum=0.9, clipvalue=0.5)\n",
    "cnn_lstm_grad_clipping_model.compile(optimizer=opt, loss='mape')\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=5)\n",
    "cnn_lstm_grad_clipping_model.summary()\n",
    "\n",
    "cnn_lstm_grad_clipping_history = cnn_lstm_grad_clipping_model.fit(\n",
    "    train_x, \n",
    "    train_y, \n",
    "    validation_data=[val_x, val_y], \n",
    "    batch_size = 30, \n",
    "    shuffle = True, \n",
    "    epochs=30, \n",
    "    callbacks=[early_stopping_monitor]\n",
    ")\n",
    "\n",
    "utils.show_results(cnn_lstm_grad_clipping_history, cnn_lstm_grad_clipping_model, val_x, val_y, \"Sequential CNN-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.layers import Input, LSTM\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "from see_rnn import get_gradients, features_0D, features_1D, features_2D\n",
    "\n",
    "# def make_model(rnn_layer, batch_shape, units):\n",
    "#     ipt = Input(batch_shape=batch_shape)\n",
    "#     x   = rnn_layer(units, activation='tanh', return_sequences=True)(ipt)\n",
    "#     out = rnn_layer(units, activation='tanh', return_sequences=False)(x)\n",
    "#     model = Model(ipt, out)\n",
    "#     model.compile(Adam(4e-3), 'mse')\n",
    "#     return model\n",
    "    \n",
    "# def make_data(batch_shape):\n",
    "#     return np.random.randn(*batch_shape), \\\n",
    "#            np.random.uniform(-1, 1, (batch_shape[0], units))\n",
    "\n",
    "# def train_model(model, iterations, batch_shape):\n",
    "#     x, y = make_data(batch_shape)\n",
    "#     for i in range(iterations):\n",
    "#         model.train_on_batch(x, y)\n",
    "#         print(end='.')  # progbar\n",
    "#         if i % 40 == 0:\n",
    "#             x, y = make_data(batch_shape)\n",
    "\n",
    "# units = 6\n",
    "# batch_shape = (16, 100, 2*units)\n",
    "\n",
    "# model = make_model(LSTM, batch_shape, units)\n",
    "# train_model(model, 300, batch_shape)\n",
    "\n",
    "# x, y  = make_data(batch_shape)\n",
    "grads_all  = get_gradients(lstm_model, 5, val_x[:10], val_y[:10])  # return_sequences=True,  layer index 1\n",
    "# grads_last = get_gradients(model, 2, x, y)  # return_sequences=False, layer index 2\n",
    "\n",
    "features_1D(grads_all, n_rows=2)\n",
    "features_2D(grads_all, n_rows=10, norm=(-.01, .01))\n",
    "features_0D(grads_all[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todos:\n",
    "\n",
    "# Investigate exploding gradients, using clip values for gradient clipping\n",
    "# Investigate visualization strategies for LSTM and Conv Layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
